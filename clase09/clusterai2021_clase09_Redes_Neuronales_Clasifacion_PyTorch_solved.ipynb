{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHzfMHbFqIKi"
   },
   "source": [
    "# ClusterAI 2021\n",
    "\n",
    "# Ciencia de Datos - Ingeniería Industrial - UTN BA\n",
    "\n",
    "# clase_09: Practica Redes Neuronales\n",
    "\n",
    "### Elaborado por: Aguirre Nicolas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8Y4IUeNrTH4"
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zj2VEgN6qjDJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pd.set_option('display.float_format', lambda x: '%.1d' % x) # Para acotar los decimales en pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shZj2OGKs0Hl"
   },
   "source": [
    "# IRIS DATASET\n",
    "\n",
    "En esta primera ejercitacion vamos a retomar el dataset Iris (visto en la Clase 4)\n",
    "\n",
    "\n",
    "El conjunto de datos contiene 50 muestras de cada una de tres especies de Iris (Iris setosa, Iris virginica e Iris versicolor). Se midió cuatro rasgos de cada muestra: lo largo y lo ancho del sépalos y pétalos, en centímetros. Basado en la combinación de estos cuatro rasgos.\n",
    "\n",
    "https://es.wikipedia.org/wiki/Iris_flor_conjunto_de_datos\n",
    "\n",
    "Los datos son:\n",
    "\n",
    "| Columna | Descripcion |\n",
    "| --- | --- |\n",
    "| ID | Unique ID |\n",
    "| SepalLengthCm | Length of the sepal (cm) |\n",
    "| SepalWidthCm | Width of the sepal (cm) |\n",
    "| PetalLengthCm | Length of the petal (cm) |\n",
    "| PetalWidthCm | Width of the petal (cm) |\n",
    "| Species | name |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxUnNijVsmww"
   },
   "outputs": [],
   "source": [
    "# Primero cargamos los datos que ya vienen incluidos en la libreria sk-learn.\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIbBihoeU95Q"
   },
   "outputs": [],
   "source": [
    "n_features = np.shape(X)[0]\n",
    "n_samples = np.shape(X)[1]\n",
    "n_classes = np.unique(Y)\n",
    "print(f'Features: ',n_samples)\n",
    "print(f'Samples: ',n_features)\n",
    "print(f'Classes: ',n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hhudQ_Xs1yL"
   },
   "outputs": [],
   "source": [
    "# Veamos la primera sample\n",
    "print(f'X: {X[0]}')\n",
    "print(f'Y: {Y[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--P7frv5qX51"
   },
   "source": [
    "## SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSP2IQWiq91_"
   },
   "outputs": [],
   "source": [
    "# Separamos train y test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "# Separamos train y validation set\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFMeW20jqa52"
   },
   "source": [
    "## SCALING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UReu6invWAGg"
   },
   "outputs": [],
   "source": [
    "# Noralizamos\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_norm = scaler.transform(x_train)\n",
    "x_val_norm = scaler.transform(x_val)\n",
    "x_test_norm = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPCxvrLMtFYc"
   },
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dv5XmStNuTP0"
   },
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "print('Version de Pytorch: ',torch.__version__)\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset,Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4wHwXuVtPg9"
   },
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJfJ3XbtEfoa"
   },
   "source": [
    "Al igual que TensorFlow-Keras, PyTorch es una libreria para codear modelos de NN y tambien tiene su modelo Sequential.\n",
    "\n",
    "Pero a diferencia de TF-Keras, PyTorch no tiene nativamente las funciones **fit**, **evaluate** y **predict**.\n",
    "\n",
    "Por lo cual somos nosotros quienes vamos a tener codear el entrenamiento ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_L2XfJcEF9Q"
   },
   "source": [
    "El tipo de arquitectura que vamos a utilizar es una NN *fully-conected*. Para esto utilizaremos la funcion *nn.Sequential()* a la cual le pasaremos los distintos componentes de la red.\n",
    "\n",
    "Cuando usamos una arquitectura secuencial, vamos construyendo el **foreward pass** de la red de manera tal que la red utiliza la salida de la capa inmediatamente anterior en la capa siguiente, y asi sucecivamente hasta llegar al ultimo elemento de la red con el cual generamos el output.\n",
    "\n",
    "Puntualmente, en este ejemplo utilizaremos unicamente dos componentes.\n",
    "\n",
    "* [**nn.Linear**](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) : Para aplicar la transformacion lineal $z = xW^T + b$\n",
    "\n",
    "* [**nn.Sigmoid**](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) : Para aplicar la funcion $\\sigma(x) = \\frac{1}{1 + \\exp(-z)}$\n",
    "\n",
    "En donde $W^T$ son los pesos (weights) que aprendera la red.\n",
    "\n",
    "Cada una de las capas de nn.Linear se construye dandole la informacion de la cantidad de features de entrada y de salida. Como en nuestro ejemplo tenemos 3 clases de flores, la ultima capa debera tener una salida de dimension = 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdKlnGr2tFEd"
   },
   "outputs": [],
   "source": [
    "input_features = 4\n",
    "layers_features = 4\n",
    "output_dim = 3\n",
    "model = nn.Sequential(\n",
    "          nn.Linear(input_features,layers_features),\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear(layers_features,layers_features),\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear(layers_features,layers_features),\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear(layers_features,output_dim)\n",
    "          )         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90n1GM9SvPUK"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlSnpr9ICHOA"
   },
   "source": [
    "## Optimizador y Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QH9v8SqvYuX"
   },
   "outputs": [],
   "source": [
    "# Learning rate y Optimizador\n",
    "lr = 0.05\n",
    "optimizador = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "# Funcion de penalizacion\n",
    "loss_func = nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQqCg3n74MbG"
   },
   "source": [
    "# Tensor Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xse7iLVFH84T"
   },
   "source": [
    "En general, cuando entrenamos NN los datos deben estar contenidos en tensores. \n",
    "Un tensor es una generalización de los vectores y las matrices y se entiende fácilmente como una matriz multidimensional. \n",
    "\n",
    "Ejemplo:\n",
    "Un vector es un 1D tensor de dimension $1x5$ --> \n",
    "Una matriz es un 2D tensor de dimension $2x5$\n",
    "\n",
    "En particular, dentro del area de Deep Learning se llama Tensor a aquellas estructura de datos que son capaces de realizar operaciones en paralelo dentro de las GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrWi1A7GwDFB"
   },
   "outputs": [],
   "source": [
    "# Batch size\n",
    "bs = 8\n",
    "# Pasamos nuestro numpy arrays, a Tensor\n",
    "x_train_norm_t, y_train_t = torch.Tensor(x_train_norm), torch.LongTensor(y_train) # Los Long Tensor son int\n",
    "x_val_norm_t, y_val_t = torch.Tensor(x_val_norm), torch.LongTensor(y_val)\n",
    "x_test_norm_t, y_test_t = torch.Tensor(x_test_norm), torch.LongTensor(y_test)\n",
    "# Creamos los Dataset\n",
    "train_ds = TensorDataset(x_train_norm_t,y_train_t)\n",
    "val_ds = TensorDataset(x_val_norm_t,y_val_t) \n",
    "test_ds = TensorDataset(x_test_norm_t,y_test_t) \n",
    "# Creamos los Dataloader\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=bs, shuffle=True) \n",
    "test_dl = DataLoader(test_ds, batch_size=bs, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCFJnJ86Bpmt"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v65OgSpRGM0q"
   },
   "source": [
    "En este punto ya contamos con todas las \"herramientras\" que necesitamos para entrenar un modelo de NN.\n",
    "\n",
    "* Datos\n",
    "* Optimizador\n",
    "* Funcion de Penalizacion\n",
    "* Red Neuronal\n",
    "\n",
    "Vamos a ver como es que esto elementos \"interactuan\" para actualizar los parametros/*weights* de la Red Neuronal ... es decir ... para entrenarla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAwFcarcVMsH"
   },
   "source": [
    "![](https://drive.google.com/uc?id=1d7Rq5FsmQNWcP8T1KGi_TodL49jCEKnY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NapSjQIkwDD4"
   },
   "outputs": [],
   "source": [
    "# Cantidad de Epochs\n",
    "n_epochs = 900\n",
    "\n",
    "# Generamos un diccionario donde vamos a guaradr historial de entrenamiento\n",
    "training = {'train':{'loss':[],                             \n",
    "                    'acc':[]},\n",
    "           'val':{'loss':[],\n",
    "                  'acc':[]}\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJx1qPJ3wDAu"
   },
   "outputs": [],
   "source": [
    "# For loop epochs\n",
    "for epoch in range(n_epochs):\n",
    "  # Ahora nosotros vamos a tener que definir estas variables ...\n",
    "  train_loss = 0\n",
    "  cls_correctas_train = 0\n",
    "  train_acc = 0\n",
    "  ################################################\n",
    "  # TRAINING\n",
    "  ################################################\n",
    "  for i , (x_batch, y_batch) in enumerate(train_dl):\n",
    "    # Limpiamos todos los gradientes cargados en el optimizador\n",
    "    optimizador.zero_grad()\n",
    "    # Con un x_batch generamos una prediccion\n",
    "    y_pred = model(x_batch)\n",
    "    # La clase predicha será el índice de máximo valor luego del \n",
    "    # softmax (integrado en CrossEntropyLoss)\n",
    "    _, predicted = torch.max(y_pred.data, 1)\n",
    "    # Calculamos la loss\n",
    "    batch_loss = loss_func(y_pred,y_batch)\n",
    "    # Calculamos el gradiente de la loss\n",
    "    # Aca es donde sucede el back-propagation a.k.a. \"la magia\"\n",
    "    batch_loss.backward() \n",
    "    # Ajustamos los parametros del modelo con el optimizador\n",
    "    optimizador.step()\n",
    "    # Acumulamos la loss\n",
    "    train_loss += batch_loss.item()\n",
    "    # Sumamos la cantidad de clases correctas en el batch\n",
    "    correct_i = (predicted == y_batch).sum().item()\n",
    "    # Acumulamos corrects\n",
    "    cls_correctas_train += correct_i\n",
    "\n",
    "  ################################################\n",
    "  # VALIDATION\n",
    "  ################################################\n",
    "  val_loss = 0\n",
    "  cls_correctas_val = 0\n",
    "  val_acc = 0\n",
    "  with torch.torch.inference_mode():\n",
    "    for x_batch, y_batch in val_dl:\n",
    "      y_pred = model(x_batch)\n",
    "      _, predicted = torch.max(y_pred.data, 1)\n",
    "      batch_loss = loss_func(y_pred,y_batch)\n",
    "      val_loss += batch_loss.item()\n",
    "      correct_i = (predicted == y_batch).sum().item()\n",
    "      cls_correctas_val += correct_i\n",
    "\n",
    "  # Calculamos el accuracy\n",
    "  train_acc = (cls_correctas_train / len(train_ds))*100    # Calculamos el accuracy\n",
    "  val_acc = (cls_correctas_val / len(val_ds))*100  \n",
    "\n",
    "  # Imprimimos en pantalla\n",
    "  print('Epoch: {} T_Loss: {:.4f} T_Acc: {:.2f}%  V_Loss: {:.4f} V_Acc: {:.2f} %'.format(\n",
    "      epoch,train_loss,train_acc,val_loss,val_acc))\n",
    "\n",
    "  # Guardamos en el historial de entrenamiento ...\n",
    "  training['train']['loss'].append(train_loss)\n",
    "  training['train']['acc'].append(train_acc)\n",
    "  training['val']['loss'].append(val_loss)\n",
    "  training['val']['acc'].append(val_acc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcTsBG9UAYJe"
   },
   "outputs": [],
   "source": [
    "#Loss\n",
    "train_loss_h = training['train']['loss']\n",
    "val_loss_h = training['val']['loss']\n",
    "# Acc\n",
    "train_acc_h = training['train']['acc']\n",
    "val_acc_h = training['val']['acc']\n",
    "lepochs = range(1, len(train_loss_h) + 1)\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(16,6))\n",
    "axs[0].plot(lepochs, train_loss_h, 'b', label='Train loss')\n",
    "axs[0].plot(lepochs, val_loss_h, 'r', label='Val loss')\n",
    "axs[0].set_title('Training and validation Loss',fontsize=20)\n",
    "axs[0].legend(fontsize=16)\n",
    "axs[1].plot(lepochs, train_acc_h, 'b', label='Train Accuracy')\n",
    "axs[1].plot(lepochs, val_acc_h, 'r', label='Validation Accuracy')\n",
    "axs[1].set_title('Training and validation Loss',fontsize=20)\n",
    "axs[1].legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79yqU-Uunu5O"
   },
   "source": [
    "# **PREGUNTAS**:\n",
    "```\n",
    "1) Es correcto que nuestra loss de train y de validacion disminuyan, y sin embargo el accuracy se mantenga igual/baje ? Por que ? \n",
    "\n",
    "2) Que cambios podriamos hacer para intentar solucionar el entrenamiento con la sigmoid function?\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "US03jef4exdE"
   },
   "source": [
    "# DESAFIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvxWJk1ve6VV"
   },
   "source": [
    "Generar una funcion llamada **func_train()** y **func_val()** para reemplezar dentro del loop de entrenamiento.\n",
    "\n",
    "Luego, generar una fucion llamada **func_fit()** la cual reemplace todo el entrenamiento, y validacion.\n",
    "\n",
    "Ayuda: func_fit() debe recibir como argumentos (inputs)\n",
    "\n",
    "* el modelo, optimizador, loss function, dataloaders, datasets y la cantidad de epochs a entrenar\n",
    "\n",
    "* el return de la funcion es el modelo ya entrenado y el historial de entrenamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oF8i9BUie-kD"
   },
   "outputs": [],
   "source": [
    "def func_train(model, train_dl,train_ds, loss_func, optimizador):\n",
    "  train_loss = 0\n",
    "  cls_correctas_train = 0\n",
    "  train_acc = 0\n",
    "  ################################################\n",
    "  # TRAINING\n",
    "  ################################################\n",
    "  for i , (x_batch, y_batch) in enumerate(train_dl):\n",
    "    # Limpiamos todos los gradientes cargados en el optimizador\n",
    "    optimizador.zero_grad()\n",
    "    # Con un x_batch generamos una prediccion\n",
    "    y_pred = model(x_batch)\n",
    "    # La clase predicha será el índice de máximo valor luego del \n",
    "    # softmax (integrado en CrossEntropyLoss)\n",
    "    _, predicted = torch.max(y_pred.data, 1)\n",
    "    # Calculamos la loss\n",
    "    batch_loss = loss_func(y_pred,y_batch)\n",
    "    # Calculamos el gradiente de la loss\n",
    "    # Aca es donde sucede el back-propagation a.k.a. \"la magia\"\n",
    "    batch_loss.backward() \n",
    "    # Ajustamos los parametros del modelo con el optimizador\n",
    "    optimizador.step()\n",
    "    # Acumulamos la loss\n",
    "    train_loss += batch_loss.item()\n",
    "    # Sumamos la cantidad de clases correctas en el batch\n",
    "    correct_i = (predicted == y_batch).sum().item()\n",
    "    # Acumulamos corrects\n",
    "    cls_correctas_train += correct_i\n",
    "  # Calculamos el accuracy\n",
    "  train_acc = (cls_correctas_train / len(train_ds))*100    # Calculamos el accuracy\n",
    "\n",
    "  return (model, train_loss,  cls_correctas_train,  train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GNJUtIYyhkzO"
   },
   "outputs": [],
   "source": [
    "def func_validation(model, val_dl,val_ds,loss_func):\n",
    "  val_loss = 0\n",
    "  cls_correctas_val = 0\n",
    "  val_acc = 0\n",
    "  ################################################\n",
    "  # VALIDATION\n",
    "  ################################################\n",
    "  with torch.torch.inference_mode():\n",
    "    for x_batch, y_batch in val_dl:\n",
    "      y_pred = model(x_batch)\n",
    "      _, predicted = torch.max(y_pred.data, 1)\n",
    "      batch_loss = loss_func(y_pred,y_batch)\n",
    "      val_loss += batch_loss.item()\n",
    "      correct_i = (predicted == y_batch).sum().item()\n",
    "      cls_correctas_val += correct_i\n",
    "  # Calculamos el accuracy\n",
    "  val_acc = (cls_correctas_val / len(val_ds))*100\n",
    "  return (val_loss, cls_correctas_val, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Ufv-Lyqhms-"
   },
   "outputs": [],
   "source": [
    "def func_fit(n_epochs,model,\n",
    "             train_dl,train_ds,val_dl,val_ds,\n",
    "             loss_func,optimizador):\n",
    "  \n",
    "  # Generamos un diccionario donde vamos a guaradr historial de entrenamiento\n",
    "  training = {'train':{'loss':[],                             \n",
    "                      'acc':[]},\n",
    "            'val':{'loss':[],\n",
    "                    'acc':[]}\n",
    "            }\n",
    "  # For loop epochs\n",
    "  for epoch in range(n_epochs):\n",
    "    model,train_loss, cls_correctas_train, train_acc =  func_train(model, train_dl,train_ds, loss_func, optimizador)\n",
    "    val_loss, cls_correctas_val, val_acc = func_validation(model, val_dl,val_ds,loss_func)\n",
    "    # Imprimimos en pantalla\n",
    "    print('Epoch: {} T_Loss: {:.4f} T_Acc: {:.2f}%  V_Loss: {:.4f} V_Acc: {:.2f} %'.format(\n",
    "        epoch,train_loss,train_acc,val_loss,val_acc))\n",
    "\n",
    "    # Guardamos en el historial de entrenamiento ...\n",
    "    training['train']['loss'].append(train_loss)\n",
    "    training['train']['acc'].append(train_acc)\n",
    "    training['val']['loss'].append(val_loss)\n",
    "    training['val']['acc'].append(val_acc)\n",
    "  return (model,training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77gbWM6CiwAs"
   },
   "outputs": [],
   "source": [
    "# Cantidad de Epochs\n",
    "n_epochs = 800 \n",
    "model , training =  func_fit( n_epochs,model,train_dl,train_ds,val_dl,val_ds,loss_func,optimizador)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULAa37jNjq8q"
   },
   "source": [
    "# Creacion del Modelos \n",
    "## (Avanzado y No Obligatorio para el curso !)\n",
    "En la siguiente seccion veremos como se definen los modelos de NN de un modo mas avanzado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0D7l-1x0lLw6"
   },
   "outputs": [],
   "source": [
    "class Red_Neuronal_Personalizada(nn.Module):\n",
    "    def __init__(self,input_features,layers_features,output_dim):\n",
    "        super(Red_Neuronal_Personalizada, self).__init__()\n",
    "        # Primera Capa\n",
    "        self.layer_1 = nn.Linear(input_features,layers_features)\n",
    "        self.act_1 = nn.Sigmoid()\n",
    "        # Segunda Capa\n",
    "        self.layer_2 = nn.Linear(layers_features,layers_features)\n",
    "        self.act_2 = nn.Sigmoid()\n",
    "        # Tercera Capa\n",
    "        self.layer_3 = nn.Linear(layers_features,layers_features)\n",
    "        self.act_3 = nn.Sigmoid()\n",
    "        # Ultima Capa\n",
    "        self.layer_4 = nn.Linear(layers_features,output_dim)    \n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.act_1(self.layer_1(x))\n",
    "        z = self.act_2(self.layer_2(z))\n",
    "        z = self.act_3(self.layer_3(z))\n",
    "        output = self.layer_4(z)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4L3Sk8EtlQ94"
   },
   "source": [
    "Esto a primera vista puede ser un poco abrumador, pero vayamos viendo de a poco que es lo que hace cada parte del codigo.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "class Red_Neuronal_Personalizada(nn.Module):\n",
    "```\n",
    "En esta parte definimos una clase que llamamos **Red_Neuronal_Personalizada**, que nos permitira luego crear unaobjetos/instancias de dicha clase. \n",
    "\n",
    "```\n",
    "modelo = Red_Neuronal_Personalizada()\n",
    "```\n",
    "Es similiar a cuando definimos una funcion, solo que es con objetos de PyThon.\n",
    "\n",
    "```\n",
    "def func_propia(): \n",
    "\n",
    "class clase_propia(): \n",
    "```\n",
    "\n",
    "Con respecto a la siguiente parte del código\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Red_Neuronal_Personalizada, self).__init__()\n",
    "\n",
    "Simplemente le estamos diciendo de que partes/atributos estará compuesta nuestra clase/modelo cuando lo creemos. \n",
    "\n",
    "La funcion __init__ es una funcion interna de Python que se ejecuta cuando creamos una instancia de nuestro objeto. Es decir, cuando ejecutemos \n",
    "\n",
    "```\n",
    "modelo = Red_Neuronal_Personalizada()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4kuQ6KulfZI"
   },
   "source": [
    "\n",
    "Hasta este punto las partes de nuestro modelo aun no las hemos definido, asi que es lo que haremos en la siguientes lineas:\n",
    "\n",
    "```\n",
    "class Red_Neuronal_Personalizada(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Red_Neuronal_Personalizada, self).__init__()\n",
    "        # Primera Capa\n",
    "        self.layer_1 = nn.Linear(input_features,layers_features)\n",
    "        self.act_1 = nn.Sigmoid()\n",
    "        # Segunda Capa\n",
    "        self.layer_2 = nn.Linear(layers_features,layers_features)\n",
    "        self.act_2 = nn.Sigmoid()\n",
    "        # Tercera Capa\n",
    "        self.layer_3 = nn.Linear(layers_features,layers_features)\n",
    "        self.act_3 = nn.Sigmoid()\n",
    "        # Ultima Capa\n",
    "        self.layer_4 = n.Linear(layers_features,output_dim)\n",
    "```\n",
    "\n",
    "* self. : Siempre que definimos las partes (formalmente llamado **atributo**) de las que estara compuesta un objeto en python, se **TIENE** que anteponer la palabra **self.nombre_parte**. De esta manera, cuando querramos que nuestro objeto utilice de alguna manera dicha parte, lo indicaremos de la misma manera: **self.nombre_parte()**\n",
    "\n",
    "En este ejemplo concreto, nuestro modelo / objeto tendra 7 componentes:\n",
    "\n",
    "* Layers 1, 2, 3 y 4\n",
    "* Activaciones 1, 2 y 3\n",
    "\n",
    "Todas estas componentes son objetos que importamos de la libreria PyTorch.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcYWFrw0uGUF"
   },
   "source": [
    "Veamos la siguiente linea de nuestro codigo:\n",
    "\n",
    "`def forward(self, x):`\n",
    "\n",
    "En esta seccion definiremos el forward pass nosotros mismo. Como vemos, forward es una funcion que pertenecera a nuestro objeto y se utilizara cuando durante el entrenamiento pasemos la siguiente info.\n",
    "\n",
    "`prediccion = model (x_batch)` **x_batch** en esta parte hace referencia a la **x** en:\n",
    "\n",
    "\n",
    "`def forward(self, x):`\n",
    "\n",
    "\n",
    "y *self*, se lo pasaremos SIEMPRE que definimos una funcion en nuestro objeto que necesitemos que tenga acceso a los atributos que definimos en la seccion ____ init ____\n",
    "\n",
    "Bueno, sabiendo ya lo que haremos en esta funcion, pasemos a la siguiente parte del codigo ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBk0ZHR0wKN9"
   },
   "source": [
    "    def forward(self, x):\n",
    "        z = self.act_1(self.layer_1(x))\n",
    "        z = self.act_2(self.layer_2(z))\n",
    "        z = self.act_3(self.layer_3(z))\n",
    "        output = self.layer_4(z))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcuY6FB0wMM0"
   },
   "source": [
    "En cada una de las lineas, obtenemos la salida de cada una de las capas. Fijense en particular que llamamos a las partes/atributos con **self.**.\n",
    "\n",
    "En particular : \n",
    "\n",
    "`z = self.act_1(self.layer_1(x))`\n",
    "\n",
    "Equivale a $z = \\sigma ( W \\cdot X + b)$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMGV76WawvA-"
   },
   "source": [
    "Con esto ultimo quedo definida la arquitectura de nuestra red!\n",
    "\n",
    "Vamos ahora a crear una instancia!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2DobiImxwav"
   },
   "source": [
    "Si observamos nuevamente en la funcion que genera los objetos\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "def __init__(self,input_features,layers_features,output_dim):\n",
    "```\n",
    "\n",
    "Le pasamos los argumentos:\n",
    "\n",
    "* input_features\n",
    "* layers_features\n",
    "* output_dim\n",
    "\n",
    "Asi que vamos a definirlos para crear una intancia llamada **modelo_personalizado** de nuestra clase **Red_Neuronal_Personalizada**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vbs50MUflbnA"
   },
   "outputs": [],
   "source": [
    "input_features = 4\n",
    "layers_features = 4\n",
    "output_dim = 3\n",
    "modelo_personalizado = Red_Neuronal_Personalizada(input_features,layers_features,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zfimw41hyUkG"
   },
   "outputs": [],
   "source": [
    "type(modelo_personalizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNE7Ox0I09z7"
   },
   "outputs": [],
   "source": [
    "modelo_personalizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRSXzvUNy1Sr"
   },
   "source": [
    "Listo! \n",
    "\n",
    "Ahora entendemos que es lo que suecede internamente en **nn.Sequential()**, con la gran diferencia que ahora tenemos plena capacidad para definir las partes de nuestra red y el **forward pass**.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "clusterai2021_clase09_Redes_Neuronales_Clasifacion_PyTorch_solved",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
